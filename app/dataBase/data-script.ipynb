{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered CSV saved as pokedex_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify your input and output CSV filenames\n",
    "input_csv = \"pokedex.csv\"\n",
    "output_csv = \"pokedex_filtered.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# List the columns you want to remove\n",
    "# Adjust this list based on which columns you don't need\n",
    "columns_to_drop = [\n",
    "    \"german_name\", \"japanese_name\", \"is_sub_legendary\", \"species\",\n",
    "    \"is_mythical\", \"abilities_number\", \"ability_hidden\", \"catch_rate\",\n",
    "    \"base_experience\", \"egg_type_number\", \"egg_type_2\", \"egg_type_1\",    \n",
    "    \"percentage_male\", \"egg_cycles\",\"growth_rate\"\n",
    "]\n",
    "\n",
    "# Drop the specified columns (if they exist in the DataFrame)\n",
    "df_filtered = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "df_filtered.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Filtered CSV saved as {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as pokedex_merged.csv\n"
     ]
    }
   ],
   "source": [
    "# File names (update if necessary)\n",
    "filtered_csv = \"pokedex_filtered.csv\"\n",
    "evolution_csv = \"evolution.csv\"\n",
    "output_csv = \"pokedex_merged.csv\"\n",
    "\n",
    "# Load the filtered CSV and the evolution CSV\n",
    "df_filtered = pd.read_csv(filtered_csv)\n",
    "df_evolution = pd.read_csv(evolution_csv)\n",
    "\n",
    "# Rename the \"Unnamed: 0\" column to \"id\" (if it exists)\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df.rename(columns={\"Unnamed: 0\": \"id\"}, inplace=True)\n",
    "\n",
    "# Normalize the name columns by converting them to lowercase for matching\n",
    "df_filtered['name_lower'] = df_filtered['name'].str.lower()\n",
    "df_evolution['name_lower'] = df_evolution['Name'].str.lower()\n",
    "\n",
    "# Merge the two DataFrames on the normalized name column\n",
    "df_merged = pd.merge(df_filtered, df_evolution[['name_lower', 'Evolution']], on='name_lower', how='left')\n",
    "\n",
    "# Optionally, remove the helper column\n",
    "df_merged.drop(columns=['name_lower'], inplace=True)\n",
    "\n",
    "# Function to determine if a Pokémon is mega and create a base name accordingly\n",
    "def get_form(name):\n",
    "    if \"Mega\" in name:\n",
    "        return \"mega\"\n",
    "    return \"base\"\n",
    "\n",
    "def get_base_name(row):\n",
    "    if row['form'] == \"mega\":\n",
    "        # Remove the \"Mega \" prefix to get the base name\n",
    "        return row['name'].replace(\"Mega \", \"\")\n",
    "    return None\n",
    "\n",
    "# Create a new column 'form' based on the name\n",
    "df_merged['form'] = df_merged['name'].apply(get_form)\n",
    "\n",
    "# Create a new column 'base_name' only if the form is mega\n",
    "df_merged['base_name'] = df_merged.apply(get_base_name, axis=1)\n",
    "\n",
    "# Save the updated CSV\n",
    "df_merged.to_csv(\"pokedex_final.csv\", index=False)\n",
    "print(f\"Merged CSV saved as {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, Literal, BNode, Namespace\n",
    "from rdflib.namespace import RDF, XSD, RDFS, FOAF\n",
    "\n",
    "# Define Namespaces\n",
    "PDX = Namespace(\"http://poked-x.org/pokemon/\")\n",
    "SCHEMA = Namespace(\"http://schema.org/\")\n",
    "\n",
    "# Create an RDF graph and bind namespaces for clarity\n",
    "g = Graph()\n",
    "g.bind(\"pdx\", PDX)\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "\n",
    "# Read CSV using pandas\n",
    "df = pd.read_csv(\"pokedex_final.csv\")\n",
    "\n",
    "# Build a mapping from normalized Pokémon names to their resource URIs.\n",
    "pokemon_uri_map = {}\n",
    "for idx, row in df.iterrows():\n",
    "    uid = str(row[\"Unnamed: 0\"]).strip()\n",
    "    poke_uri = URIRef(EX[\"Pokemon/\" + uid])\n",
    "    name_norm = str(row[\"name\"]).strip().lower()\n",
    "    pokemon_uri_map[name_norm] = poke_uri\n",
    "\n",
    "# First pass: create Pokémon resources and add basic properties, types, abilities, and effectiveness\n",
    "for idx, row in df.iterrows():\n",
    "    uid = str(row[\"Unnamed: 0\"]).strip()\n",
    "    name = str(row[\"name\"]).strip()\n",
    "    poke_uri = URIRef(EX[\"Pokemon/\" + uid])\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Explicit typing: mark as a Pokémon\n",
    "        g.add((poke_uri, RDF.type, EX.Pokemon))\n",
    "        \n",
    "        # Add the Pokédex number property if available\n",
    "        if pd.notna(row[\"pokedex_number\"]):\n",
    "            g.add((poke_uri, EX.pokedexNumber, Literal(int(row[\"pokedex_number\"]), datatype=XSD.integer)))\n",
    "    \n",
    "        # Basic properties\n",
    "        g.add((poke_uri, SCHEMA.name, Literal(row[\"name\"])))\n",
    "        g.add((poke_uri, EX.generation, Literal(int(row[\"generation\"]), datatype=XSD.integer)))\n",
    "        try:\n",
    "            is_leg = bool(int(row[\"is_legendary\"]))\n",
    "        except:\n",
    "            is_leg = False\n",
    "\n",
    "        g.add((poke_uri, EX.isLegendary, Literal(is_leg, datatype=XSD.boolean)))\n",
    "        g.add((poke_uri, EX.height, Literal(float(row[\"height_m\"]), datatype=XSD.float)))\n",
    "        g.add((poke_uri, EX.weight, Literal(float(row[\"weight_kg\"]), datatype=XSD.float)))\n",
    "\n",
    "        if pd.notna(row[\"total_points\"]):\n",
    "            total_points = int(row[\"total_points\"])\n",
    "            g.add((poke_uri, EX.totalPoints, Literal(total_points, datatype=XSD.integer)))\n",
    "        else:\n",
    "            # Optionally, add a default value or skip this triple.\n",
    "            g.add((poke_uri, EX.totalPoints, Literal(0, datatype=XSD.integer)))\n",
    "\n",
    "        if pd.notna(row[\"hp\"]):\n",
    "            hp = int(row[\"hp\"])\n",
    "            g.add((poke_uri, EX.hp, Literal(hp, datatype=XSD.integer)))\n",
    "        else:\n",
    "            # Optionally, add a default value or skip this triple.\n",
    "            g.add((poke_uri, EX.hp, Literal(0, datatype=XSD.integer)))\n",
    "        \n",
    "\n",
    "        g.add((poke_uri, EX.attack, Literal(int(row[\"attack\"]), datatype=XSD.integer)))\n",
    "        g.add((poke_uri, EX.defense, Literal(int(row[\"defense\"]), datatype=XSD.integer)))\n",
    "        g.add((poke_uri, EX.spAttack, Literal(int(row[\"sp_attack\"]), datatype=XSD.integer)))\n",
    "        g.add((poke_uri, EX.spDefense, Literal(int(row[\"sp_defense\"]), datatype=XSD.integer)))\n",
    "        g.add((poke_uri, EX.speed, Literal(int(row[\"speed\"]), datatype=XSD.integer)))\n",
    "\n",
    "        bf = row[\"base_friendship\"]\n",
    "        if pd.notna(bf):\n",
    "            g.add((poke_uri, EX.baseFriendship, Literal(int(bf), datatype=XSD.integer)))\n",
    "        else:\n",
    "            # Optionally, you could assign a default value or simply skip adding the triple.\n",
    "            # Here, we'll assign a default value of 0.\n",
    "            g.add((poke_uri, EX.baseFriendship, Literal(0, datatype=XSD.integer)))\n",
    "        \n",
    "        # Link to Type resources\n",
    "        type1 = str(row[\"type_1\"]).strip()\n",
    "        if type1:\n",
    "            type1_uri = URIRef(EX[\"Type/\" + type1.lower()])\n",
    "            g.add((poke_uri, EX.primaryType, type1_uri))\n",
    "            g.add((type1_uri, RDFS.label, Literal(type1)))\n",
    "        type2 = str(row[\"type_2\"]).strip()\n",
    "        if type2:\n",
    "            type2_uri = URIRef(EX[\"Type/\" + type2.lower()])\n",
    "            g.add((poke_uri, EX.secondaryType, type2_uri))\n",
    "            g.add((type2_uri, RDFS.label, Literal(type2)))\n",
    "        \n",
    "        # Link to Ability resources (skip if missing or NaN)\n",
    "        ability1 = row[\"ability_1\"]\n",
    "        if pd.notna(ability1) and str(ability1).strip().lower() != \"nan\":\n",
    "            ability1_str = str(ability1).strip()\n",
    "            ability1_uri = URIRef(EX[\"Ability/\" + ability1_str.lower().replace(\" \", \"_\")])\n",
    "            g.add((poke_uri, EX.ability1, ability1_uri))\n",
    "            g.add((ability1_uri, RDFS.label, Literal(ability1_str)))\n",
    "        ability2 = row[\"ability_2\"]\n",
    "        if pd.notna(ability2) and str(ability2).strip().lower() != \"nan\":\n",
    "            ability2_str = str(ability2).strip()\n",
    "            ability2_uri = URIRef(EX[\"Ability/\" + ability2_str.lower().replace(\" \", \"_\")])\n",
    "            g.add((poke_uri, EX.ability2, ability2_uri))\n",
    "            g.add((ability2_uri, RDFS.label, Literal(ability2_str)))\n",
    "        \n",
    "        # Effectiveness multipliers using a blank node\n",
    "        eff_node = BNode()\n",
    "        g.add((poke_uri, EX.effectiveness, eff_node))\n",
    "        effectiveness_attrs = [\n",
    "            (\"against_normal\", \"againstNormal\"),\n",
    "            (\"against_fire\", \"againstFire\"),\n",
    "            (\"against_water\", \"againstWater\"),\n",
    "            (\"against_electric\", \"againstElectric\"),\n",
    "            (\"against_grass\", \"againstGrass\"),\n",
    "            (\"against_ice\", \"againstIce\"),\n",
    "            (\"against_fight\", \"againstFight\"),\n",
    "            (\"against_poison\", \"againstPoison\"),\n",
    "            (\"against_ground\", \"againstGround\"),\n",
    "            (\"against_flying\", \"againstFlying\"),\n",
    "            (\"against_psychic\", \"againstPsychic\"),\n",
    "            (\"against_bug\", \"againstBug\"),\n",
    "            (\"against_rock\", \"againstRock\"),\n",
    "            (\"against_ghost\", \"againstGhost\"),\n",
    "            (\"against_dragon\", \"againstDragon\"),\n",
    "            (\"against_dark\", \"againstDark\"),\n",
    "            (\"against_steel\", \"againstSteel\"),\n",
    "            (\"against_fairy\", \"againstFairy\")\n",
    "        ]\n",
    "        for csv_attr, prop_local in effectiveness_attrs:\n",
    "            val = row[csv_attr]\n",
    "            g.add((eff_node, EX[prop_local], Literal(float(val), datatype=XSD.float)))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Pokémon {name} (ID: {uid}): {e}\")\n",
    "    continue\n",
    "\n",
    "\n",
    "# Second pass: Add evolution and mega evolution relationships\n",
    "for idx, row in df.iterrows():\n",
    "\n",
    "    uid = str(row[\"Unnamed: 0\"]).strip()\n",
    "    name = str(row[\"name\"]).strip()\n",
    "    poke_uri = URIRef(EX[\"Pokemon/\" + uid])\n",
    "    \n",
    "    try:\n",
    "        # Evolution: link to the next evolution if provided\n",
    "        evolution = str(row[\"Evolution\"]).strip().lower()\n",
    "        if evolution:\n",
    "            target_uri = pokemon_uri_map.get(evolution)\n",
    "            if target_uri:\n",
    "                g.add((poke_uri, EX.evolvesTo, target_uri))\n",
    "        \n",
    "        # Mega Evolution: if the form indicates mega or base_name is provided and differs from the current name\n",
    "        form = str(row[\"form\"]).strip().lower()\n",
    "        base_name = str(row[\"base_name\"]).strip().lower()\n",
    "        name_norm = str(row[\"name\"]).strip().lower()\n",
    "        if form == \"mega\" or (base_name and base_name != name_norm):\n",
    "            base_uri = pokemon_uri_map.get(base_name)\n",
    "            if base_uri:\n",
    "                g.add((poke_uri, EX.megaEvolutionOf, base_uri))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing evolution for Pokémon {name} (ID: {uid}): {e}\")\n",
    "    continue\n",
    "\n",
    "# Serialize the RDF graph in Turtle format and display it\n",
    "turtle_data = g.serialize(format=\"turtle\")\n",
    "with open(\"pokemon-rdf.ttl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(turtle_data.decode(\"utf-8\") if isinstance(turtle_data, bytes) else turtle_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Missing image for ID 194: 154-mega\n",
      "❌ Missing image for ID 242: 201\n",
      "❌ Missing image for ID 460: 386\n",
      "❌ Missing image for ID 461: 386\n",
      "❌ Missing image for ID 462: 386\n",
      "❌ Missing image for ID 463: 386\n",
      "❌ Missing image for ID 489: 412\n",
      "❌ Missing image for ID 490: 413\n",
      "❌ Missing image for ID 491: 413\n",
      "❌ Missing image for ID 492: 413\n",
      "❌ Missing image for ID 500: 421\n",
      "❌ Missing image for ID 501: 422\n",
      "❌ Missing image for ID 502: 423\n",
      "❌ Missing image for ID 576: 487\n",
      "❌ Missing image for ID 577: 487\n",
      "❌ Missing image for ID 582: 492\n",
      "❌ Missing image for ID 583: 492\n",
      "❌ Missing image for ID 584: 493\n",
      "❌ Missing image for ID 642: 550\n",
      "❌ Missing image for ID 643: 550\n",
      "❌ Missing image for ID 649: 555\n",
      "❌ Missing image for ID 650: 555\n",
      "❌ Missing image for ID 651: 555\n",
      "❌ Missing image for ID 652: 555\n",
      "❌ Missing image for ID 683: 585\n",
      "❌ Missing image for ID 684: 586\n",
      "❌ Missing image for ID 740: 641\n",
      "❌ Missing image for ID 741: 641\n",
      "❌ Missing image for ID 742: 642\n",
      "❌ Missing image for ID 743: 642\n",
      "❌ Missing image for ID 746: 645\n",
      "❌ Missing image for ID 747: 645\n",
      "❌ Missing image for ID 751: 647\n",
      "❌ Missing image for ID 752: 647\n",
      "❌ Missing image for ID 841: 722\n",
      "❌ Missing image for ID 842: 723\n",
      "❌ Missing image for ID 843: 724\n",
      "❌ Missing image for ID 844: 725\n",
      "❌ Missing image for ID 845: 726\n",
      "❌ Missing image for ID 846: 727\n",
      "❌ Missing image for ID 847: 728\n",
      "❌ Missing image for ID 848: 729\n",
      "❌ Missing image for ID 849: 730\n",
      "❌ Missing image for ID 850: 731\n",
      "❌ Missing image for ID 851: 732\n",
      "❌ Missing image for ID 852: 733\n",
      "❌ Missing image for ID 853: 734\n",
      "❌ Missing image for ID 854: 735\n",
      "❌ Missing image for ID 855: 736\n",
      "❌ Missing image for ID 856: 737\n",
      "❌ Missing image for ID 857: 738\n",
      "❌ Missing image for ID 858: 739\n",
      "❌ Missing image for ID 859: 740\n",
      "❌ Missing image for ID 860: 741\n",
      "❌ Missing image for ID 861: 741\n",
      "❌ Missing image for ID 862: 741\n",
      "❌ Missing image for ID 863: 741\n",
      "❌ Missing image for ID 864: 742\n",
      "❌ Missing image for ID 865: 743\n",
      "❌ Missing image for ID 866: 744\n",
      "❌ Missing image for ID 867: 744\n",
      "❌ Missing image for ID 868: 745\n",
      "❌ Missing image for ID 869: 745\n",
      "❌ Missing image for ID 870: 745\n",
      "❌ Missing image for ID 871: 746\n",
      "❌ Missing image for ID 872: 746\n",
      "❌ Missing image for ID 873: 747\n",
      "❌ Missing image for ID 874: 748\n",
      "❌ Missing image for ID 875: 749\n",
      "❌ Missing image for ID 876: 750\n",
      "❌ Missing image for ID 877: 751\n",
      "❌ Missing image for ID 878: 752\n",
      "❌ Missing image for ID 879: 753\n",
      "❌ Missing image for ID 880: 754\n",
      "❌ Missing image for ID 881: 755\n",
      "❌ Missing image for ID 882: 756\n",
      "❌ Missing image for ID 883: 757\n",
      "❌ Missing image for ID 884: 758\n",
      "❌ Missing image for ID 885: 759\n",
      "❌ Missing image for ID 886: 760\n",
      "❌ Missing image for ID 887: 761\n",
      "❌ Missing image for ID 888: 762\n",
      "❌ Missing image for ID 889: 763\n",
      "❌ Missing image for ID 890: 764\n",
      "❌ Missing image for ID 891: 765\n",
      "❌ Missing image for ID 892: 766\n",
      "❌ Missing image for ID 893: 767\n",
      "❌ Missing image for ID 894: 768\n",
      "❌ Missing image for ID 895: 769\n",
      "❌ Missing image for ID 896: 770\n",
      "❌ Missing image for ID 897: 771\n",
      "❌ Missing image for ID 898: 772\n",
      "❌ Missing image for ID 899: 773\n",
      "❌ Missing image for ID 900: 774\n",
      "❌ Missing image for ID 901: 774\n",
      "❌ Missing image for ID 902: 775\n",
      "❌ Missing image for ID 903: 776\n",
      "❌ Missing image for ID 904: 777\n",
      "❌ Missing image for ID 905: 778\n",
      "❌ Missing image for ID 906: 779\n",
      "❌ Missing image for ID 907: 780\n",
      "❌ Missing image for ID 908: 781\n",
      "❌ Missing image for ID 909: 782\n",
      "❌ Missing image for ID 910: 783\n",
      "❌ Missing image for ID 911: 784\n",
      "❌ Missing image for ID 912: 785\n",
      "❌ Missing image for ID 913: 786\n",
      "❌ Missing image for ID 914: 787\n",
      "❌ Missing image for ID 915: 788\n",
      "❌ Missing image for ID 916: 789\n",
      "❌ Missing image for ID 917: 790\n",
      "❌ Missing image for ID 918: 791\n",
      "❌ Missing image for ID 919: 792\n",
      "❌ Missing image for ID 920: 793\n",
      "❌ Missing image for ID 921: 794\n",
      "❌ Missing image for ID 922: 795\n",
      "❌ Missing image for ID 923: 796\n",
      "❌ Missing image for ID 924: 797\n",
      "❌ Missing image for ID 925: 798\n",
      "❌ Missing image for ID 926: 799\n",
      "❌ Missing image for ID 927: 800\n",
      "❌ Missing image for ID 928: 800\n",
      "❌ Missing image for ID 929: 800\n",
      "❌ Missing image for ID 930: 800\n",
      "❌ Missing image for ID 931: 801\n",
      "❌ Missing image for ID 932: 802\n",
      "❌ Missing image for ID 933: 803\n",
      "❌ Missing image for ID 934: 804\n",
      "❌ Missing image for ID 935: 805\n",
      "❌ Missing image for ID 936: 806\n",
      "❌ Missing image for ID 937: 807\n",
      "❌ Missing image for ID 938: 808\n",
      "❌ Missing image for ID 939: 809\n",
      "❌ Missing image for ID 940: 810\n",
      "❌ Missing image for ID 941: 811\n",
      "❌ Missing image for ID 942: 812\n",
      "❌ Missing image for ID 943: 813\n",
      "❌ Missing image for ID 944: 814\n",
      "❌ Missing image for ID 945: 815\n",
      "❌ Missing image for ID 946: 816\n",
      "❌ Missing image for ID 947: 817\n",
      "❌ Missing image for ID 948: 818\n",
      "❌ Missing image for ID 949: 819\n",
      "❌ Missing image for ID 950: 820\n",
      "❌ Missing image for ID 951: 821\n",
      "❌ Missing image for ID 952: 822\n",
      "❌ Missing image for ID 953: 823\n",
      "❌ Missing image for ID 954: 824\n",
      "❌ Missing image for ID 955: 825\n",
      "❌ Missing image for ID 956: 826\n",
      "❌ Missing image for ID 957: 827\n",
      "❌ Missing image for ID 958: 828\n",
      "❌ Missing image for ID 959: 829\n",
      "❌ Missing image for ID 960: 830\n",
      "❌ Missing image for ID 961: 831\n",
      "❌ Missing image for ID 962: 832\n",
      "❌ Missing image for ID 963: 833\n",
      "❌ Missing image for ID 964: 834\n",
      "❌ Missing image for ID 965: 835\n",
      "❌ Missing image for ID 966: 836\n",
      "❌ Missing image for ID 967: 837\n",
      "❌ Missing image for ID 968: 838\n",
      "❌ Missing image for ID 969: 839\n",
      "❌ Missing image for ID 970: 840\n",
      "❌ Missing image for ID 971: 841\n",
      "❌ Missing image for ID 972: 842\n",
      "❌ Missing image for ID 973: 843\n",
      "❌ Missing image for ID 974: 844\n",
      "❌ Missing image for ID 975: 845\n",
      "❌ Missing image for ID 976: 846\n",
      "❌ Missing image for ID 977: 847\n",
      "❌ Missing image for ID 978: 848\n",
      "❌ Missing image for ID 979: 849\n",
      "❌ Missing image for ID 980: 849\n",
      "❌ Missing image for ID 981: 850\n",
      "❌ Missing image for ID 982: 851\n",
      "❌ Missing image for ID 983: 852\n",
      "❌ Missing image for ID 984: 853\n",
      "❌ Missing image for ID 985: 854\n",
      "❌ Missing image for ID 986: 855\n",
      "❌ Missing image for ID 987: 856\n",
      "❌ Missing image for ID 988: 857\n",
      "❌ Missing image for ID 989: 858\n",
      "❌ Missing image for ID 990: 859\n",
      "❌ Missing image for ID 991: 860\n",
      "❌ Missing image for ID 992: 861\n",
      "❌ Missing image for ID 993: 862\n",
      "❌ Missing image for ID 994: 863\n",
      "❌ Missing image for ID 995: 864\n",
      "❌ Missing image for ID 996: 865\n",
      "❌ Missing image for ID 997: 866\n",
      "❌ Missing image for ID 998: 867\n",
      "❌ Missing image for ID 999: 868\n",
      "❌ Missing image for ID 1000: 869\n",
      "❌ Missing image for ID 1001: 870\n",
      "❌ Missing image for ID 1002: 871\n",
      "❌ Missing image for ID 1003: 872\n",
      "❌ Missing image for ID 1004: 873\n",
      "❌ Missing image for ID 1005: 874\n",
      "❌ Missing image for ID 1006: 875\n",
      "❌ Missing image for ID 1007: 875\n",
      "❌ Missing image for ID 1008: 876\n",
      "❌ Missing image for ID 1009: 876\n",
      "❌ Missing image for ID 1010: 877\n",
      "❌ Missing image for ID 1011: 877\n",
      "❌ Missing image for ID 1012: 878\n",
      "❌ Missing image for ID 1013: 879\n",
      "❌ Missing image for ID 1014: 880\n",
      "❌ Missing image for ID 1015: 881\n",
      "❌ Missing image for ID 1016: 882\n",
      "❌ Missing image for ID 1017: 883\n",
      "❌ Missing image for ID 1018: 884\n",
      "❌ Missing image for ID 1019: 885\n",
      "❌ Missing image for ID 1020: 886\n",
      "❌ Missing image for ID 1021: 887\n",
      "❌ Missing image for ID 1022: 888\n",
      "❌ Missing image for ID 1023: 888\n",
      "❌ Missing image for ID 1024: 889\n",
      "❌ Missing image for ID 1025: 889\n",
      "❌ Missing image for ID 1026: 890\n",
      "❌ Missing image for ID 1027: 890\n",
      "\n",
      "✅ Done! Updated dataset saved to: pokedex_with_images.csv\n",
      "Renamed images saved to: pokemon-images-index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- CONFIG ---\n",
    "image_folder = \"pokemon-images\"         # <- Change to your actual path\n",
    "output_folder = \"pokemon-images-index\"        # <- Where renamed images will be saved\n",
    "csv_file = \"pokedex_final.csv\"\n",
    "output_csv = \"pokedex_with_images.csv\"  # <- Updated dataset with has_image column\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create output directory if needed\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Create a new column to track image existence\n",
    "df[\"has_image\"] = False\n",
    "\n",
    "# Loop through each Pokémon\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        dataset_id = str(row[\"Unnamed: 0\"]).strip()\n",
    "        pokedex_number = str(int(row[\"pokedex_number\"]))\n",
    "        form = str(row[\"form\"]).strip().lower()\n",
    "\n",
    "        # Determine expected filename\n",
    "        if form.startswith(\"mega\"):\n",
    "            original_filename = f\"{pokedex_number}-{form}.png\"  # e.g., 6-mega-x.png\n",
    "        else:\n",
    "            original_filename = f\"{pokedex_number}.png\"\n",
    "\n",
    "        old_path = os.path.join(image_folder, original_filename)\n",
    "        new_filename = f\"{dataset_id}.png\"\n",
    "        new_path = os.path.join(output_folder, new_filename)\n",
    "\n",
    "        if os.path.exists(old_path):\n",
    "            shutil.copy2(old_path, new_path)\n",
    "            df.at[idx, \"has_image\"] = True\n",
    "        else:\n",
    "            print(f\"❌ Missing image for {dataset_id}: {original_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing row {idx} ({row['name']}): {e}\")\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"\\n✅ Done! Updated dataset saved to: {output_csv}\")\n",
    "print(f\"Renamed images saved to: {output_folder}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
