{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered CSV saved as pokedex_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify your input and output CSV filenames\n",
    "input_csv = \"pokedex.csv\"\n",
    "output_csv = \"pokedex_filtered.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# List the columns you want to remove\n",
    "# Adjust this list based on which columns you don't need\n",
    "columns_to_drop = [\n",
    "    \"german_name\", \"japanese_name\", \"is_sub_legendary\", \"species\",\n",
    "    \"is_mythical\", \"abilities_number\", \"ability_hidden\", \"catch_rate\",\n",
    "    \"base_experience\", \"egg_type_number\", \"egg_type_2\", \"egg_type_1\",    \n",
    "    \"percentage_male\", \"egg_cycles\",\"growth_rate\"\n",
    "]\n",
    "\n",
    "# Drop the specified columns (if they exist in the DataFrame)\n",
    "df_filtered = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "df_filtered.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Filtered CSV saved as {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as pokedex_merged.csv\n"
     ]
    }
   ],
   "source": [
    "# File names (update if necessary)\n",
    "filtered_csv = \"pokedex_filtered.csv\"\n",
    "evolution_csv = \"evolution.csv\"\n",
    "output_csv = \"pokedex_merged.csv\"\n",
    "\n",
    "# Load the filtered CSV and the evolution CSV\n",
    "df_filtered = pd.read_csv(filtered_csv)\n",
    "df_evolution = pd.read_csv(evolution_csv)\n",
    "\n",
    "# Rename the \"Unnamed: 0\" column to \"id\" (if it exists)\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df.rename(columns={\"Unnamed: 0\": \"id\"}, inplace=True)\n",
    "\n",
    "# Normalize the name columns by converting them to lowercase for matching\n",
    "df_filtered['name_lower'] = df_filtered['name'].str.lower()\n",
    "df_evolution['name_lower'] = df_evolution['Name'].str.lower()\n",
    "\n",
    "# Merge the two DataFrames on the normalized name column\n",
    "df_merged = pd.merge(df_filtered, df_evolution[['name_lower', 'Evolution']], on='name_lower', how='left')\n",
    "\n",
    "# Optionally, remove the helper column\n",
    "df_merged.drop(columns=['name_lower'], inplace=True)\n",
    "\n",
    "# Function to determine if a Pokémon is mega and create a base name accordingly\n",
    "def get_form(name):\n",
    "    if \"Mega\" in name:\n",
    "        return \"mega\"\n",
    "    return \"base\"\n",
    "\n",
    "def get_base_name(row):\n",
    "    if row['form'] == \"mega\":\n",
    "        # Remove the \"Mega \" prefix to get the base name\n",
    "        return row['name'].replace(\"Mega \", \"\")\n",
    "    return None\n",
    "\n",
    "# Create a new column 'form' based on the name\n",
    "df_merged['form'] = df_merged['name'].apply(get_form)\n",
    "\n",
    "# Create a new column 'base_name' only if the form is mega\n",
    "df_merged['base_name'] = df_merged.apply(get_base_name, axis=1)\n",
    "\n",
    "# Save the updated CSV\n",
    "df_merged.to_csv(\"pokedex_final.csv\", index=False)\n",
    "print(f\"Merged CSV saved as {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, Literal, BNode, Namespace\n",
    "from rdflib.namespace import RDF, XSD, RDFS, FOAF\n",
    "\n",
    "# Define Namespaces\n",
    "PDX = Namespace(\"http://poked-x.org/pokemon/\")\n",
    "SCHEMA = Namespace(\"http://schema.org/\")\n",
    "\n",
    "# Create an RDF graph and bind namespaces for clarity\n",
    "g = Graph()\n",
    "g.bind(\"pdx\", PDX)\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "\n",
    "# Read CSV using pandas\n",
    "df = pd.read_csv(\"pokedex_with_images.csv\")\n",
    "\n",
    "# Build a mapping from normalized Pokémon names to their resource URIs.\n",
    "pokemon_uri_map = {}\n",
    "for idx, row in df.iterrows():\n",
    "    uid = str(row[\"Unnamed: 0\"]).strip()\n",
    "    poke_uri = URIRef(PDX[\"Pokemon/\" + uid])\n",
    "    name_norm = str(row[\"name\"]).strip().lower()\n",
    "    pokemon_uri_map[name_norm] = poke_uri\n",
    "\n",
    "# First pass: create Pokémon resources and add basic properties, types, abilities, and effectiveness\n",
    "for idx, row in df.iterrows():\n",
    "    uid = str(row[\"Unnamed: 0\"]).strip()\n",
    "    name = str(row[\"name\"]).strip()\n",
    "    poke_uri = URIRef(PDX[\"Pokemon/\" + uid])\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Explicit typing: mark as a Pokémon\n",
    "        #g.add((poke_uri, RDF.type, PDX.Pokemon))\n",
    "        \n",
    "        # Add the Pokédex number property if available\n",
    "        if pd.notna(row[\"pokedex_number\"]):\n",
    "            g.add((poke_uri, PDX.pokedexNumber, Literal(int(row[\"pokedex_number\"]), datatype=XSD.integer)))\n",
    "    \n",
    "        # Basic properties\n",
    "        g.add((poke_uri, PDX.name, Literal(row[\"name\"])))\n",
    "        g.add((poke_uri, PDX.generation, Literal(int(row[\"generation\"]), datatype=XSD.integer)))\n",
    "        try:\n",
    "            is_leg = bool(int(row[\"is_legendary\"]))\n",
    "        except:\n",
    "            is_leg = False\n",
    "\n",
    "        g.add((poke_uri, PDX.isLegendary, Literal(is_leg, datatype=XSD.boolean)))\n",
    "        g.add((poke_uri, PDX.height, Literal(float(row[\"height_m\"]), datatype=XSD.float)))\n",
    "        g.add((poke_uri, PDX.weight, Literal(float(row[\"weight_kg\"]), datatype=XSD.float)))\n",
    "\n",
    "        if pd.notna(row[\"total_points\"]):\n",
    "            total_points = int(row[\"total_points\"])\n",
    "            g.add((poke_uri, PDX.totalPoints, Literal(total_points, datatype=XSD.integer)))\n",
    "        else:\n",
    "            # Optionally, add a default value or skip this triple.\n",
    "            g.add((poke_uri, PDX.totalPoints, Literal(0, datatype=XSD.integer)))\n",
    "\n",
    "        if pd.notna(row[\"hp\"]):\n",
    "            hp = int(row[\"hp\"])\n",
    "            g.add((poke_uri, PDX.hp, Literal(hp, datatype=XSD.integer)))\n",
    "        else:\n",
    "            # Optionally, add a default value or skip this triple.\n",
    "            g.add((poke_uri, PDX.hp, Literal(0, datatype=XSD.integer)))\n",
    "        \n",
    "\n",
    "        g.add((poke_uri, PDX.attack, Literal(int(row[\"attack\"]), datatype=XSD.integer)))\n",
    "        g.add((poke_uri, PDX.defense, Literal(int(row[\"defense\"]), datatype=XSD.integer)))\n",
    "        g.add((poke_uri, PDX.spAttack, Literal(int(row[\"sp_attack\"]), datatype=XSD.integer)))\n",
    "        g.add((poke_uri, PDX.spDefense, Literal(int(row[\"sp_defense\"]), datatype=XSD.integer)))\n",
    "        g.add((poke_uri, PDX.speed, Literal(int(row[\"speed\"]), datatype=XSD.integer)))\n",
    "\n",
    "        bf = row[\"base_friendship\"]\n",
    "        if pd.notna(bf):\n",
    "            g.add((poke_uri, PDX.baseFriendship, Literal(int(bf), datatype=XSD.integer)))\n",
    "        else:\n",
    "            # Optionally, you could assign a default value or simply skip adding the triple.\n",
    "            # Here, we'll assign a default value of 0.\n",
    "            g.add((poke_uri, PDX.baseFriendship, Literal(0, datatype=XSD.integer)))\n",
    "        \n",
    "        # Link to Type resources\n",
    "        type1 = str(row[\"type_1\"]).strip()\n",
    "        if type1:\n",
    "            type1_uri = URIRef(PDX[\"Type/\" + type1.lower()])\n",
    "            g.add((poke_uri, PDX.primaryType, type1_uri))\n",
    "            g.add((type1_uri, RDFS.label, Literal(type1)))\n",
    "        type2 = str(row[\"type_2\"]).strip()\n",
    "        if pd.notna(type2) and type2.lower() != \"nan\" and type2:\n",
    "            # Ensure type2 is not NaN or empty\n",
    "            type2_uri = URIRef(PDX[\"Type/\" + type2.lower()])\n",
    "            g.add((poke_uri, PDX.secondaryType, type2_uri))\n",
    "            g.add((type2_uri, RDFS.label, Literal(type2)))\n",
    "        \n",
    "        # Link to Ability resources (skip if missing or NaN)\n",
    "        ability1 = row[\"ability_1\"]\n",
    "        if pd.notna(ability1) and str(ability1).strip().lower() != \"nan\":\n",
    "            ability1_str = str(ability1).strip()\n",
    "            ability1_uri = URIRef(PDX[\"Ability/\" + ability1_str.lower().replace(\" \", \"_\")])\n",
    "            g.add((poke_uri, PDX.ability1, ability1_uri))\n",
    "            g.add((ability1_uri, RDFS.label, Literal(ability1_str)))\n",
    "        ability2 = row[\"ability_2\"]\n",
    "        if pd.notna(ability2) and str(ability2).strip().lower() != \"nan\":\n",
    "            ability2_str = str(ability2).strip()\n",
    "            ability2_uri = URIRef(PDX[\"Ability/\" + ability2_str.lower().replace(\" \", \"_\")])\n",
    "            g.add((poke_uri, PDX.ability2, ability2_uri))\n",
    "            g.add((ability2_uri, RDFS.label, Literal(ability2_str)))\n",
    "        \n",
    "         # Effectiveness profile\n",
    "        effectiveness_node = BNode()\n",
    "        g.add((poke_uri, PDX.hasEffectiveness, effectiveness_node))\n",
    "        g.add((effectiveness_node, RDF.type, PDX.Effectiveness))\n",
    "        \n",
    "        vulnerability_attrs = {\n",
    "            \"against_normal\": \"againstNormal\",\n",
    "            \"against_fire\": \"againstFire\",\n",
    "            \"against_water\": \"againstWater\",\n",
    "            \"against_electric\": \"againstElectric\",\n",
    "            \"against_grass\": \"againstGrass\",\n",
    "            \"against_ice\": \"againstIce\",\n",
    "            \"against_fight\": \"againstFight\",\n",
    "            \"against_poison\": \"againstPoison\",\n",
    "            \"against_ground\": \"againstGround\",\n",
    "            \"against_flying\": \"againstFlying\",\n",
    "            \"against_psychic\": \"againstPsychic\",\n",
    "            \"against_bug\": \"againstBug\",\n",
    "            \"against_rock\": \"againstRock\",\n",
    "            \"against_ghost\": \"againstGhost\",\n",
    "            \"against_dragon\": \"againstDragon\",\n",
    "            \"against_dark\": \"againstDark\",\n",
    "            \"against_steel\": \"againstSteel\",\n",
    "            \"against_fairy\": \"againstFairy\"\n",
    "        }\n",
    "        for csv_attr, ont_prop in vulnerability_attrs.items():\n",
    "            if pd.notna(row[csv_attr]):\n",
    "                g.add((effectiveness_node, PDX[ont_prop], Literal(float(row[csv_attr]), datatype=XSD.float)))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Pokémon {name} (ID: {uid}): {e}\")\n",
    "    continue\n",
    "\n",
    "\n",
    "# Second pass: Add evolution and mega evolution relationships\n",
    "for idx, row in df.iterrows():\n",
    "\n",
    "    uid = str(row[\"Unnamed: 0\"]).strip()\n",
    "    name = str(row[\"name\"]).strip()\n",
    "    poke_uri = URIRef(PDX[\"Pokemon/\" + uid])\n",
    "    \n",
    "    try:\n",
    "        # Evolution: link to the next evolution if provided\n",
    "        evolution = str(row[\"Evolution\"]).strip().lower()\n",
    "        if evolution:\n",
    "            target_uri = pokemon_uri_map.get(evolution)\n",
    "            if target_uri:\n",
    "                g.add((poke_uri, PDX.evolvesTo, target_uri))\n",
    "        \n",
    "        # Mega Evolution: if the form indicates mega or base_name is provided and differs from the current name\n",
    "        form = str(row[\"form\"]).strip().lower()\n",
    "        base_name = str(row[\"base_name\"]).strip().lower()\n",
    "        name_norm = str(row[\"name\"]).strip().lower()\n",
    "        if form == \"mega\" or (base_name and base_name != name_norm):\n",
    "            base_uri = pokemon_uri_map.get(base_name)\n",
    "            if base_uri:\n",
    "                g.add((poke_uri, PDX.megaEvolutionOf, base_uri))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing evolution for Pokémon {name} (ID: {uid}): {e}\")\n",
    "    continue\n",
    "\n",
    "# Serialize the RDF graph in Turtle format and display it\n",
    "turtle_data = g.serialize(format=\"turtle\")\n",
    "with open(\"pokemon-data-aligned-new.ttl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(turtle_data.decode(\"utf-8\") if isinstance(turtle_data, bytes) else turtle_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Missing image for 194: 154-mega.png\n",
      "❌ Missing image for 242: 201.png\n",
      "❌ Missing image for 460: 386.png\n",
      "❌ Missing image for 461: 386.png\n",
      "❌ Missing image for 462: 386.png\n",
      "❌ Missing image for 463: 386.png\n",
      "❌ Missing image for 489: 412.png\n",
      "❌ Missing image for 490: 413.png\n",
      "❌ Missing image for 491: 413.png\n",
      "❌ Missing image for 492: 413.png\n",
      "❌ Missing image for 500: 421.png\n",
      "❌ Missing image for 501: 422.png\n",
      "❌ Missing image for 502: 423.png\n",
      "❌ Missing image for 576: 487.png\n",
      "❌ Missing image for 577: 487.png\n",
      "❌ Missing image for 582: 492.png\n",
      "❌ Missing image for 583: 492.png\n",
      "❌ Missing image for 584: 493.png\n",
      "❌ Missing image for 642: 550.png\n",
      "❌ Missing image for 643: 550.png\n",
      "❌ Missing image for 649: 555.png\n",
      "❌ Missing image for 650: 555.png\n",
      "❌ Missing image for 651: 555.png\n",
      "❌ Missing image for 652: 555.png\n",
      "❌ Missing image for 683: 585.png\n",
      "❌ Missing image for 684: 586.png\n",
      "❌ Missing image for 740: 641.png\n",
      "❌ Missing image for 741: 641.png\n",
      "❌ Missing image for 742: 642.png\n",
      "❌ Missing image for 743: 642.png\n",
      "❌ Missing image for 746: 645.png\n",
      "❌ Missing image for 747: 645.png\n",
      "❌ Missing image for 751: 647.png\n",
      "❌ Missing image for 752: 647.png\n",
      "❌ Missing image for 841: 722.png\n",
      "❌ Missing image for 842: 723.png\n",
      "❌ Missing image for 843: 724.png\n",
      "❌ Missing image for 844: 725.png\n",
      "❌ Missing image for 845: 726.png\n",
      "❌ Missing image for 846: 727.png\n",
      "❌ Missing image for 847: 728.png\n",
      "❌ Missing image for 848: 729.png\n",
      "❌ Missing image for 849: 730.png\n",
      "❌ Missing image for 850: 731.png\n",
      "❌ Missing image for 851: 732.png\n",
      "❌ Missing image for 852: 733.png\n",
      "❌ Missing image for 853: 734.png\n",
      "❌ Missing image for 854: 735.png\n",
      "❌ Missing image for 855: 736.png\n",
      "❌ Missing image for 856: 737.png\n",
      "❌ Missing image for 857: 738.png\n",
      "❌ Missing image for 858: 739.png\n",
      "❌ Missing image for 859: 740.png\n",
      "❌ Missing image for 860: 741.png\n",
      "❌ Missing image for 861: 741.png\n",
      "❌ Missing image for 862: 741.png\n",
      "❌ Missing image for 863: 741.png\n",
      "❌ Missing image for 864: 742.png\n",
      "❌ Missing image for 865: 743.png\n",
      "❌ Missing image for 866: 744.png\n",
      "❌ Missing image for 867: 744.png\n",
      "❌ Missing image for 868: 745.png\n",
      "❌ Missing image for 869: 745.png\n",
      "❌ Missing image for 870: 745.png\n",
      "❌ Missing image for 871: 746.png\n",
      "❌ Missing image for 872: 746.png\n",
      "❌ Missing image for 873: 747.png\n",
      "❌ Missing image for 874: 748.png\n",
      "❌ Missing image for 875: 749.png\n",
      "❌ Missing image for 876: 750.png\n",
      "❌ Missing image for 877: 751.png\n",
      "❌ Missing image for 878: 752.png\n",
      "❌ Missing image for 879: 753.png\n",
      "❌ Missing image for 880: 754.png\n",
      "❌ Missing image for 881: 755.png\n",
      "❌ Missing image for 882: 756.png\n",
      "❌ Missing image for 883: 757.png\n",
      "❌ Missing image for 884: 758.png\n",
      "❌ Missing image for 885: 759.png\n",
      "❌ Missing image for 886: 760.png\n",
      "❌ Missing image for 887: 761.png\n",
      "❌ Missing image for 888: 762.png\n",
      "❌ Missing image for 889: 763.png\n",
      "❌ Missing image for 890: 764.png\n",
      "❌ Missing image for 891: 765.png\n",
      "❌ Missing image for 892: 766.png\n",
      "❌ Missing image for 893: 767.png\n",
      "❌ Missing image for 894: 768.png\n",
      "❌ Missing image for 895: 769.png\n",
      "❌ Missing image for 896: 770.png\n",
      "❌ Missing image for 897: 771.png\n",
      "❌ Missing image for 898: 772.png\n",
      "❌ Missing image for 899: 773.png\n",
      "❌ Missing image for 900: 774.png\n",
      "❌ Missing image for 901: 774.png\n",
      "❌ Missing image for 902: 775.png\n",
      "❌ Missing image for 903: 776.png\n",
      "❌ Missing image for 904: 777.png\n",
      "❌ Missing image for 905: 778.png\n",
      "❌ Missing image for 906: 779.png\n",
      "❌ Missing image for 907: 780.png\n",
      "❌ Missing image for 908: 781.png\n",
      "❌ Missing image for 909: 782.png\n",
      "❌ Missing image for 910: 783.png\n",
      "❌ Missing image for 911: 784.png\n",
      "❌ Missing image for 912: 785.png\n",
      "❌ Missing image for 913: 786.png\n",
      "❌ Missing image for 914: 787.png\n",
      "❌ Missing image for 915: 788.png\n",
      "❌ Missing image for 916: 789.png\n",
      "❌ Missing image for 917: 790.png\n",
      "❌ Missing image for 918: 791.png\n",
      "❌ Missing image for 919: 792.png\n",
      "❌ Missing image for 920: 793.png\n",
      "❌ Missing image for 921: 794.png\n",
      "❌ Missing image for 922: 795.png\n",
      "❌ Missing image for 923: 796.png\n",
      "❌ Missing image for 924: 797.png\n",
      "❌ Missing image for 925: 798.png\n",
      "❌ Missing image for 926: 799.png\n",
      "❌ Missing image for 927: 800.png\n",
      "❌ Missing image for 928: 800.png\n",
      "❌ Missing image for 929: 800.png\n",
      "❌ Missing image for 930: 800.png\n",
      "❌ Missing image for 931: 801.png\n",
      "❌ Missing image for 932: 802.png\n",
      "❌ Missing image for 933: 803.png\n",
      "❌ Missing image for 934: 804.png\n",
      "❌ Missing image for 935: 805.png\n",
      "❌ Missing image for 936: 806.png\n",
      "❌ Missing image for 937: 807.png\n",
      "❌ Missing image for 938: 808.png\n",
      "❌ Missing image for 939: 809.png\n",
      "❌ Missing image for 940: 810.png\n",
      "❌ Missing image for 941: 811.png\n",
      "❌ Missing image for 942: 812.png\n",
      "❌ Missing image for 943: 813.png\n",
      "❌ Missing image for 944: 814.png\n",
      "❌ Missing image for 945: 815.png\n",
      "❌ Missing image for 946: 816.png\n",
      "❌ Missing image for 947: 817.png\n",
      "❌ Missing image for 948: 818.png\n",
      "❌ Missing image for 949: 819.png\n",
      "❌ Missing image for 950: 820.png\n",
      "❌ Missing image for 951: 821.png\n",
      "❌ Missing image for 952: 822.png\n",
      "❌ Missing image for 953: 823.png\n",
      "❌ Missing image for 954: 824.png\n",
      "❌ Missing image for 955: 825.png\n",
      "❌ Missing image for 956: 826.png\n",
      "❌ Missing image for 957: 827.png\n",
      "❌ Missing image for 958: 828.png\n",
      "❌ Missing image for 959: 829.png\n",
      "❌ Missing image for 960: 830.png\n",
      "❌ Missing image for 961: 831.png\n",
      "❌ Missing image for 962: 832.png\n",
      "❌ Missing image for 963: 833.png\n",
      "❌ Missing image for 964: 834.png\n",
      "❌ Missing image for 965: 835.png\n",
      "❌ Missing image for 966: 836.png\n",
      "❌ Missing image for 967: 837.png\n",
      "❌ Missing image for 968: 838.png\n",
      "❌ Missing image for 969: 839.png\n",
      "❌ Missing image for 970: 840.png\n",
      "❌ Missing image for 971: 841.png\n",
      "❌ Missing image for 972: 842.png\n",
      "❌ Missing image for 973: 843.png\n",
      "❌ Missing image for 974: 844.png\n",
      "❌ Missing image for 975: 845.png\n",
      "❌ Missing image for 976: 846.png\n",
      "❌ Missing image for 977: 847.png\n",
      "❌ Missing image for 978: 848.png\n",
      "❌ Missing image for 979: 849.png\n",
      "❌ Missing image for 980: 849.png\n",
      "❌ Missing image for 981: 850.png\n",
      "❌ Missing image for 982: 851.png\n",
      "❌ Missing image for 983: 852.png\n",
      "❌ Missing image for 984: 853.png\n",
      "❌ Missing image for 985: 854.png\n",
      "❌ Missing image for 986: 855.png\n",
      "❌ Missing image for 987: 856.png\n",
      "❌ Missing image for 988: 857.png\n",
      "❌ Missing image for 989: 858.png\n",
      "❌ Missing image for 990: 859.png\n",
      "❌ Missing image for 991: 860.png\n",
      "❌ Missing image for 992: 861.png\n",
      "❌ Missing image for 993: 862.png\n",
      "❌ Missing image for 994: 863.png\n",
      "❌ Missing image for 995: 864.png\n",
      "❌ Missing image for 996: 865.png\n",
      "❌ Missing image for 997: 866.png\n",
      "❌ Missing image for 998: 867.png\n",
      "❌ Missing image for 999: 868.png\n",
      "❌ Missing image for 1000: 869.png\n",
      "❌ Missing image for 1001: 870.png\n",
      "❌ Missing image for 1002: 871.png\n",
      "❌ Missing image for 1003: 872.png\n",
      "❌ Missing image for 1004: 873.png\n",
      "❌ Missing image for 1005: 874.png\n",
      "❌ Missing image for 1006: 875.png\n",
      "❌ Missing image for 1007: 875.png\n",
      "❌ Missing image for 1008: 876.png\n",
      "❌ Missing image for 1009: 876.png\n",
      "❌ Missing image for 1010: 877.png\n",
      "❌ Missing image for 1011: 877.png\n",
      "❌ Missing image for 1012: 878.png\n",
      "❌ Missing image for 1013: 879.png\n",
      "❌ Missing image for 1014: 880.png\n",
      "❌ Missing image for 1015: 881.png\n",
      "❌ Missing image for 1016: 882.png\n",
      "❌ Missing image for 1017: 883.png\n",
      "❌ Missing image for 1018: 884.png\n",
      "❌ Missing image for 1019: 885.png\n",
      "❌ Missing image for 1020: 886.png\n",
      "❌ Missing image for 1021: 887.png\n",
      "❌ Missing image for 1022: 888.png\n",
      "❌ Missing image for 1023: 888.png\n",
      "❌ Missing image for 1024: 889.png\n",
      "❌ Missing image for 1025: 889.png\n",
      "❌ Missing image for 1026: 890.png\n",
      "❌ Missing image for 1027: 890.png\n",
      "\n",
      "✅ Done! Updated dataset saved to: pokedex_with_images.csv\n",
      "Renamed images saved to: pokemon-images-index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- CONFIG ---\n",
    "image_folder = \"pokemon-images\"         # <- Change to your actual path\n",
    "output_folder = \"pokemon-images-index\"        # <- Where renamed images will be saved\n",
    "csv_file = \"pokedex_final.csv\"\n",
    "output_csv = \"pokedex_with_images.csv\"  # <- Updated dataset with has_image column\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create output directory if needed\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Create a new column to track image existence\n",
    "df[\"has_image\"] = False\n",
    "\n",
    "# Loop through each Pokémon\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        dataset_id = str(row[\"Unnamed: 0\"]).strip()\n",
    "        pokedex_number = str(int(row[\"pokedex_number\"]))\n",
    "        form = str(row[\"form\"]).strip().lower()\n",
    "\n",
    "        # Determine expected filename\n",
    "        if form.startswith(\"mega\"):\n",
    "            original_filename = f\"{pokedex_number}-{form}.png\"  # e.g., 6-mega-x.png\n",
    "        else:\n",
    "            original_filename = f\"{pokedex_number}.png\"\n",
    "\n",
    "        old_path = os.path.join(image_folder, original_filename)\n",
    "        new_filename = f\"{dataset_id}.png\"\n",
    "        new_path = os.path.join(output_folder, new_filename)\n",
    "\n",
    "        if os.path.exists(old_path):\n",
    "            shutil.copy2(old_path, new_path)\n",
    "            df.at[idx, \"has_image\"] = True\n",
    "        else:\n",
    "            print(f\"❌ Missing image for {dataset_id}: {original_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing row {idx} ({row['name']}): {e}\")\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"\\n✅ Done! Updated dataset saved to: {output_csv}\")\n",
    "print(f\"Renamed images saved to: {output_folder}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
